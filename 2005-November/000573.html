<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Maay-svncheckins] r601 - in trunk/maay: . sql
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/maay-svncheckins/2005-November/index.html" >
   <LINK REL="made" HREF="mailto:maay-svncheckins%40lists.berlios.de?Subject=Re%3A%20%5BMaay-svncheckins%5D%20r601%20-%20in%20trunk/maay%3A%20.%20sql&In-Reply-To=%3C200511221632.jAMGWta8018901%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000572.html">
   <LINK REL="Next"  HREF="000574.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Maay-svncheckins] r601 - in trunk/maay: . sql</H1>
    <B>adimasci at BerliOS</B> 
    <A HREF="mailto:maay-svncheckins%40lists.berlios.de?Subject=Re%3A%20%5BMaay-svncheckins%5D%20r601%20-%20in%20trunk/maay%3A%20.%20sql&In-Reply-To=%3C200511221632.jAMGWta8018901%40sheep.berlios.de%3E"
       TITLE="[Maay-svncheckins] r601 - in trunk/maay: . sql">adimasci at berlios.de
       </A><BR>
    <I>Tue Nov 22 17:32:55 CET 2005</I>
    <P><UL>
        <LI>Previous message: <A HREF="000572.html">[Maay-svncheckins] r600 - trunk
</A></li>
        <LI>Next message: <A HREF="000574.html">[Maay-svncheckins] r602 - in trunk: . maay/configuration/win32
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#573">[ date ]</a>
              <a href="thread.html#573">[ thread ]</a>
              <a href="subject.html#573">[ subject ]</a>
              <a href="author.html#573">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: adimasci
Date: 2005-11-22 17:32:54 +0100 (Tue, 22 Nov 2005)
New Revision: 601

Modified:
   trunk/maay/configuration.py
   trunk/maay/dbentity.py
   trunk/maay/p2pquerier.py
   trunk/maay/querier.py
   trunk/maay/sql/mysql.sql
   trunk/maay/webapplication.py
Log:
IMPORTANT NOTE : This is a big checkin with lot of manual merges
since Berlios was (and is still more or less) down. It might
break stuff. Please be lenient for the next 2 revisions.

This checkin :

 - fixes counters buglets on indexation page
 - adds code to download distant document from other peers
   when the main one could not fulfill the query. This
   could not be tested for a lot of reasons, so expect to have 
   the distant file download functionality broken for a few
   more revisions.



Modified: trunk/maay/configuration.py
===================================================================
--- trunk/maay/configuration.py	2005-11-22 15:59:59 UTC (rev 600)
+++ trunk/maay/configuration.py	2005-11-22 16:32:54 UTC (rev 601)
@@ -259,7 +259,7 @@
 
 def _download_dir():
     if sys.platform == 'win32':
-        theDir = r'C:\Documents and Settings\All Users\Desktop\Maay Documents\downloaded'
+        theDir = r'\Documents and Settings\All Users\Desktop\Maay Documents\downloaded'
     else:
         theDir = osp.expanduser('~/maay-downloads/')
     if not osp.exists(theDir):

Modified: trunk/maay/dbentity.py
===================================================================
--- trunk/maay/dbentity.py	2005-11-22 15:59:59 UTC (rev 600)
+++ trunk/maay/dbentity.py	2005-11-22 16:32:54 UTC (rev 601)
@@ -28,6 +28,7 @@
 
 from maay.texttool import normalizeText, WORD_MIN_LEN, WORD_MAX_LEN,\
      WORDS_RGX
+from maay.p2pquerier import NODE_ID
 
 class DBEntity:
     attributes = []
@@ -140,7 +141,6 @@
             assert attrname in self.attributes, 'Unknown attribute %s' % attrname
             setattr(self, attrname, value)
 
-
 class DocumentOrder:
     &quot;&quot;&quot;a small helper class to translate search criteria
        from crystal-clear high-level stuff to SQL junk&quot;&quot;&quot;
@@ -176,6 +176,7 @@
             res = &quot;ORDER BY DS.popularity&quot;
         return res + self.direction
 
+
 class Document(DBEntity):
     &quot;&quot;&quot;Represent a Document in the database
     A Document is different from a file, because several files can store
@@ -271,7 +272,7 @@
     #     query is quite messy
     def _selectContainingQuery(cls, words, mimetype=None, offset=0,
                                limit=None, allowPrivate=False,
-                               order=None):
+			       order=None):
         # XXX mimetype handling is a HACK. It needs to be integrated
         #     nicely in order to handle any kind of restrictions easily
         #word = WORDS_RGX.finditer(normalizeText(' '.join(words)))
@@ -284,8 +285,8 @@
         if not allowPrivate:
             restriction += &quot; AND D.state!=%s &quot;
             restrictionParams.append(cls.PRIVATE_STATE)
-        # just translate the plain-text ordering to SQL stuff
-        sql_order = order or str(DocumentOrder('publication_time'))
+	# just translate the plain-text ordering to SQL stuff
+	sql_order = order or str(DocumentOrder('publication_time'))
         # Question: what is the HAVING clause supposed to do ?
         # Answer: we select all documents containing one of the words
         # that we are looking for, group them by their identifier, and
@@ -306,7 +307,7 @@
                  &quot; %s &quot;
                  &quot;GROUP BY DS.db_document_id &quot;
                  &quot;HAVING count(DS.db_document_id) = %%s &quot;) + sql_order
-        query = query % (', '.join(['%s'] * len(words)), restriction)
+	query = query % (', '.join(['%s'] * len(words)), restriction)
         # XXX SQL: how to specify only the OFFSET ???????
         if limit or offset:
             query += &quot; LIMIT %s OFFSET %s&quot; % (limit or 50, offset)
@@ -316,16 +317,16 @@
 
     def selectContaining(cls, cursor, words, mimetype=None, offset=0,
                          limit=None, allowPrivate=False, order='publication_time',
-                         direction='down'):
+			 direction='down'):
         print &quot;Document selectContaining %s&quot; % words
         if not words:
             return []
-        doc_order = str(DocumentOrder(order, direction))
+	doc_order = str(DocumentOrder(order, direction))
         query, params = cls._selectContainingQuery(words, mimetype,
                                                    offset=offset,
                                                    limit=limit,
                                                    allowPrivate=allowPrivate,
-                                                   order=doc_order)
+						   order=doc_order)
         if query:
             cursor.execute(query, params)
             results = cursor.fetchall()
@@ -363,33 +364,35 @@
     &quot;&quot;&quot;Results are temporary relations created/destroyed on the fly
        they contain documents matching both local and distributed requests
     &quot;&quot;&quot;
-    attributes = ('document_id', 'query_id', 'mime_type',
+    attributes = ('document_id', 'query_id', 'node_id', 'mime_type',
                   'title', 'size', 'text', 'publication_time', 'url',
-                  'host', 'port', 'login', 'providers')
+                  'host', 'port', 'login', 'record_ts')
     key = ('document_id', 'query_id')
     tableName = 'results'
 
-    def _insertQuery(self):
-        &quot;&quot;&quot;generates an INSERT query according to object's state
-           also update provider count on collisions on (queryId, document_id)&quot;&quot;&quot;
-        values = ['%%(%s)s' % attr for attr in self.attributes
-                  if hasattr(self, attr)]
-        query = &quot;INSERT INTO %s (%s) VALUES (%s) &quot; % (self.tableName,
-                                                      ', '.join(self.boundAttributes()),
-                                                      ', '.join(values))
-        query += &quot;ON DUPLICATE KEY UPDATE providers=providers+1&quot;
-        return query
 
+##     def _insertQuery(self):
+##         &quot;&quot;&quot;generates an INSERT query according to object's state
+##            also update provider count on collisions on (queryId, document_id)&quot;&quot;&quot;
+##         values = ['%%(%s)s' % attr for attr in self.attributes
+##                   if hasattr(self, attr)]
+##         query = &quot;INSERT INTO %s (%s) VALUES (%s) &quot; % (self.tableName,
+##                                                       ', '.join(self.boundAttributes()),
+##                                                       ', '.join(values))
+##         query += &quot;ON DUPLICATE KEY UPDATE providers=providers+1&quot;
+##         return query
+
     def fromDocument(document, queryId, provider=None):
         stateDict = document.stateDict
         for key, value in stateDict.items():
             if key not in Result.attributes or not value:
                 del stateDict[key]
         if provider:
-            stateDict['login'], stateDict['host'], stateDict['port'] = provider
+            stateDict['login'], stateDict['node_id'], stateDict['host'], stateDict['port'] = provider
         else:
             stateDict['host'] = 'localhost'
             stateDict['port'] = 0
+            stateDict['node_id'] = NODE_ID # local node id
         stateDict['query_id'] = queryId
         return Result(**stateDict)
     fromDocument = staticmethod(fromDocument)
@@ -404,7 +407,9 @@
             where += &quot; AND host != 'localhost' &quot;
         elif onlyLocal:
             where += &quot; AND host = 'localhost' &quot;
-        query = 'SELECT %s FROM %s%s ORDER BY publication_time DESC LIMIT %s OFFSET %s' % (
+        query = 'SELECT %s FROM %s%s GROUP BY document_id ' \
+                'HAVING record_ts=MIN(record_ts) ' \
+                'ORDER BY publication_time DESC LIMIT %s OFFSET %s' % (
             ', '.join(cls.attributes),
             cls.tableName,
             where, limit, offset)
@@ -418,6 +423,7 @@
         return [cls(**dict(zip(cls.attributes, row))) for row in results]
     selectWhere = classmethod(selectWhere)
 
+
 class FileInfo(DBEntity):
     &quot;&quot;&quot;
     Attributes:
@@ -537,7 +543,7 @@
     &quot;&quot;&quot;
     tableName = 'nodes'
     attributes = ('node_id', 'ip', 'port', 'last_seen_time', 'last_sleep_time',
-                  'counter', 'claim_count', 'affinity', 'bandwidth')
+		  'counter', 'claim_count', 'affinity', 'bandwidth')
     key = ('node_id',)
 
 ## Gibberish belox might be useful later, don't delete it right now, please (auc)
@@ -558,14 +564,20 @@
 ##         return query
 
     def _selectActiveNodesQuery(cls):
-        query = cls._selectQuery()
+	query = cls._selectQuery()
         query += (&quot; WHERE node_id != %s AND last_seen_time &gt; last_sleep_time&quot;
                   &quot; ORDER BY last_seen_time DESC LIMIT %s&quot;)
-        return query
+         return query
     _selectActiveNodesQuery = classmethod(_selectActiveNodesQuery)
 
+    def _selectRegisteredNodesQuery(cls):
+        query = cls._selectQuery()
+        query += &quot; WHERE node_id != %s ORDER BY last_seen_time DESC LIMIT %s&quot;
+        return query
+    _selectRegisteredNodesQuery = classmethod(_selectRegisteredNodesQuery)
+
     def selectActive(cls, cursor, currentNodeId, maxResults):
-        query = cls._selectActiveNodesQuery()
+	query = cls._selectActiveNodesQuery()
         cursor.execute(query, (currentNodeId, maxResults))
         results = cursor.fetchall()
         return [cls(**dict(zip(cls.attributes, row))) for row in results]
@@ -598,3 +610,4 @@
     key = ('node_id', 'word')
 
     
+

Modified: trunk/maay/p2pquerier.py
===================================================================
--- trunk/maay/p2pquerier.py	2005-11-22 15:59:59 UTC (rev 600)
+++ trunk/maay/p2pquerier.py	2005-11-22 16:32:54 UTC (rev 601)
@@ -39,6 +39,7 @@
 nodeConfig.load()
 NODE_HOST = socket.gethostbyname(socket.gethostname())
 NODE_PORT = nodeConfig.rpcserver_port
+NODE_ID = nodeConfig.get_node_id()
 
 def hashIt(item, uname=''.join(platform.uname())):
     hasher = sha.sha()
@@ -164,7 +165,7 @@
 class P2pAnswer:
     def __init__(self, queryId, provider, documents):
         &quot;&quot;&quot;
-        :type provider: 3-tuple (login, IP, xmlrpc-port)
+        :type provider: 4-tuple (login, nodeID, IP, xmlrpc-port)
         &quot;&quot;&quot;
         self.queryId = queryId
         self.provider = provider
@@ -259,7 +260,7 @@
             proxy = Proxy(str(neighbor.getRpcUrl()))
             d = proxy.callRemote('distributedQuery', query.asKwargs())
             d.addCallback(self.querier.registerNodeActivity)
-            d.addErrback(sendQueryErrback(neighbor, self.querier))
+	    d.addErrback(sendQueryErrback(neighbor, self.querier))
             self._sentQueries[query.qid] = query
             print &quot;     ... sent to %s:%s %s&quot; % (neighbor.ip,
                                                  neighbor.port,
@@ -291,8 +292,9 @@
             abstract = makeAbstract(doc.text, query.getWords())
             doc.text = untagText(removeSpace(abstract))
 
-        # provider is a triple (login, IP, xmlrpc-port)
+        # provider is a 4-uple (login, node_id, IP, xmlrpc-port)
         provider = (getUserLogin(),
+                    NODE_ID,
                     socket.gethostbyname(socket.gethostname()),
                     NODE_PORT)
             
@@ -338,7 +340,7 @@
                                      query.qid,
                                      self.nodeId,
                                      answer.provider,
-                                     toSend) 
+                                     toSend)
                 d.addCallback(self.querier.registerNodeActivity)
                 d.addErrback(answerQueryErrback(query))
             except ValueError:
@@ -367,13 +369,10 @@
         querier.registerNodeInactivity(target.node_id)
     return QP
 
+
 def answerQueryErrback(target):
     ':type target: P2pQuery'
     def AP(failure):
-        &quot;&quot;&quot;Politely displays any problem (bug, unavailability) related
-        to an attempt to answer a query.
-        &quot;&quot;&quot;
         print &quot; ... problem answering the query to %s:%s, trace = %s&quot; \
               % (target.client_host, target.client_port, failure.getTraceback())
     return AP
-

Modified: trunk/maay/querier.py
===================================================================
--- trunk/maay/querier.py	2005-11-22 15:59:59 UTC (rev 600)
+++ trunk/maay/querier.py	2005-11-22 16:32:54 UTC (rev 601)
@@ -163,9 +163,9 @@
             cursor = self._cnx.cursor()
             return Document.selectContaining(cursor, words, query.filetype,
                                              query.offset, query.limit,
-                                             self.searchInPrivate,
-                                             order=query.order,
-                                             direction=query.direction)
+					     self.searchInPrivate,
+					     order=query.order,
+					     direction=query.direction)
         finally:
             traceback.print_exc()
             cursor.close()
@@ -244,8 +244,8 @@
 
     def _updateDownloadStatistics(self, document, words):
         cursor = self._cnx.cursor()
-        # what's this max(0, doc.count) below ??? it should be set initially
-        # at zero anyway ...
+	# what's this max(0, doc.count) below ??? it should be set initially
+	# at zero anyway ...
         document.download_count = max(0, document.download_count) + 1
         document.commit(cursor, update=True)
         db_document_id = document.db_document_id
@@ -298,7 +298,7 @@
         cursor.close()
 
     def registerNodeActivity(self, nodeId):
-        &quot;&quot;&quot;updates last_seen_time attribute&quot;&quot;&quot;
+	&quot;&quot;&quot;updates last_seen_time attribute&quot;&quot;&quot;
         cursor = self._cnx.cursor()
         nodes = Node.selectWhere(cursor, node_id=nodeId)
         if nodes:
@@ -353,6 +353,21 @@
                                      onlyLocal, onlyDistant, query_id=queryId)
         cursor.close()
         return results
+
+    def getProvidersFor(self, docId, queryId):
+        &quot;&quot;&quot;returns a list of couples (ip, port) corresponding to
+        each node that can provide the document.
+        queryId is needed only to avoid small conflicts with previous
+        queries that returned the same documents
+        (we don't want 'old' providers to appear in the list)
+        &quot;&quot;&quot;
+        cursor = self._cnx.cursor()
+        query = 'SELECT host, port FROM results WHERE query_id=%(queryId) ' \
+                'AND document_id=%(docId)'
+        cursor.execute(query, locals())
+        providers = cursor.fetchall()
+        cursor.close()
+        return providers
         
     def pushDocuments(self, queryId, documents, provider=None):
         &quot;&quot;&quot;push &lt;documents&gt; into the temporary table built for

Modified: trunk/maay/sql/mysql.sql
===================================================================
--- trunk/maay/sql/mysql.sql	2005-11-22 15:59:59 UTC (rev 600)
+++ trunk/maay/sql/mysql.sql	2005-11-22 16:32:54 UTC (rev 601)
@@ -123,8 +123,8 @@
   `ip` char(15) NOT NULL default '', -- to satisfy selectOrInsertWhere *
 -- FIXME: this should be unsigned smallint
   `port` smallint(11) NOT NULL default 0, -- to satisfy selectOrInsertWhere *
-  `last_seen_time` int(11) default 1,
-  `last_sleep_time` int(11) default 0,
+  `last_seen_time` int(11) default 0,
+  `last_sleep_time` int(11) default 1,
   `counter` int(11) NOT NULL default '0',
   `claim_count` float NOT NULL default '0',
   `affinity` double NOT NULL default '0',
@@ -151,6 +151,7 @@
 --  `db_document_id` varchar(40) NOT NULL,
   `document_id` varchar(40) NOT NULL default '',
   `query_id` varchar(64) NOT NULL,
+  `node_id` char(40) NOT NULL default '',
   `mime_type` varchar(40) NOT NULL default '',
   `title` varchar(255) default NULL,
   `size` int(11) default NULL,
@@ -161,8 +162,7 @@
   `port` int(11), -- check this
   `login` varchar(255),
   `record_ts` TIMESTAMP(8), -- DEFAULT NOW() is not necessary because records are not updated
-  `providers` int(11) default 1, -- number of providers for the document
-  PRIMARY KEY (`document_id`, `query_id`)
+  PRIMARY KEY (`document_id`, `query_id`, `node_id`)
 --  PRIMARY KEY (`db_document_id`, `query_id`, `host`, `port`)
 --  KEY `document_id` (`document_id`),
 --  KEY `url` (`url`)

Modified: trunk/maay/webapplication.py
===================================================================
--- trunk/maay/webapplication.py	2005-11-22 15:59:59 UTC (rev 600)
+++ trunk/maay/webapplication.py	2005-11-22 16:32:54 UTC (rev 601)
@@ -36,6 +36,7 @@
 from nevow import rend, tags, loaders, athena, inevow
 
 from logilab.common.textutils import normalize_text
+from logilab.common.compat import set
 
 from maay.querier import WEB_AVATARID
 from maay.configuration import get_path_of
@@ -111,17 +112,7 @@
 
 class IndexationPage(athena.LivePage):
     docFactory = loaders.xmlfile(get_path_of('indexationpage.html'))
-    implements(indexer.IIndexerObserver)
 
-    # share counter among instances
-    indexedDocuments = 0
-    untouchedDocuments = 0
-
-    privateDocuments = 0
-    publicDocuments = 0
-
-    querier = None
-    
     def __init__(self):
         athena.LivePage.__init__(self)
         self.indexerConfig = indexer.indexerConfig
@@ -145,58 +136,17 @@
     def updateStatus(self, message):
         self.callRemote('updateStatus', message)
 
-    def updateDocumentStats(self):
-        docCounts = IndexationPage.querier.getDocumentCount()
-        IndexationPage.privateDocuments = docCounts[Document.PRIVATE_STATE]
-        IndexationPage.publicDocuments = docCounts[Document.PUBLISHED_STATE]
+    def countersUpdated(self, old, new):
+        self.updateStatus(u'Indexation in progress - %s new documents / %s total' %
+                          (new, old + new))
 
-    def updatePrivateDocumentCount(self):
-        self.callRemote('updatePrivateDocumentCount', IndexationPage.privateDocuments)
-
-    def updatePublicDocumentCount(self):
-        self.callRemote('updatePublicDocumentCount', IndexationPage.publicDocuments)
-
-    def newDocumentIndexed(self, filename, state):
-        IndexationPage.indexedDocuments += 1
-
-        if state == Document.PRIVATE_STATE:
-            IndexationPage.privateDocuments += 1
-        elif state == Document.PUBLIC_STATE:
-            IndexationPage.publicDocuments += 1
-
-        if (IndexationPage.indexedDocuments % 10) == 0:
-            self.updateStatus(u'Indexation in progress - %s new documents / %s total'
-                % (IndexationPage.indexedDocuments, IndexationPage.indexedDocuments + IndexationPage.untouchedDocuments))
-            self.updatePrivateDocumentCount()
-            self.updatePublicDocumentCount()
-
-    def documentUntouched(self, filename):
-        IndexationPage.untouchedDocuments += 1
-        if (IndexationPage.untouchedDocuments % 10) == 0:
-            self.updateStatus(u'Indexation in progress - %s new documents / %s total'
-                % (IndexationPage.indexedDocuments, IndexationPage.indexedDocuments + IndexationPage.untouchedDocuments))
-        
     def indexationCompleted(self):
         self.updateStatus(u'Indexation finished - %s new documents / %s total'
             % (IndexationPage.indexedDocuments, IndexationPage.indexedDocuments + IndexationPage.untouchedDocuments))
-        # reset counters for the next indexation
-        IndexationPage.untouchedDocuments = 0
-        IndexationPage.indexedDocuments = 0
 
-        self.updateDocumentStats()
-        self.updatePrivateDocumentCount()
-        self.updatePublicDocumentCount()
-
-
     def render_message(self, context, data):
         return self.msg
 
-    def render_privateDocumentCount(self, context, data):
-        return IndexationPage.privateDocuments
-
-    def render_publicDocumentCount(self, context, data):
-        return IndexationPage.publicDocuments
-
     def render_alert(self, context, data):
         context.fillSlots(&quot;message&quot;, self.alertmessage)
         return context.tag
@@ -224,17 +174,33 @@
 class IndexationPageFactory(athena.LivePageFactory):
     implements(indexer.IIndexerObserver)
 
-    def newDocumentIndexed(self, filename, state):
-        for webpage in self.clients.itervalues():
-            webpage.newDocumentIndexed(filename, state)
+    def __init__(self, pageFactory):
+        athena.LivePageFactory.__init__(self, pageFactory)
+        self.untouchedDocuments = 0
+        self.indexedDocuments = 0
+
+    def newDocumentIndexed(self, filename):
+        self.indexedDocuments += 1
+        # refresh pages for each group of 10 indexed files
+        if (self.indexedDocuments % 10) == 0:
+            for webpage in self.clients.itervalues():
+                webpage.countersUpdated(self.untouchedDocuments,
+                                        self.indexedDocuments)
         
     def documentUntouched(self, filename):
-        for webpage in self.clients.itervalues():
-            webpage.documentUntouched(filename)
-        
+        self.untouchedDocuments += 1
+        if (self.untouchedDocuments % 10) == 0:
+            for webpage in self.clients.itervalues():
+                webpage.countersUpdated(self.untouchedDocuments,
+                                        self.indexedDocuments)
+    
     def indexationCompleted(self):
         for webpage in self.clients.itervalues():
             webpage.indexationCompleted()
+        # reset counters after indexation is finished
+        self.untouchedDocuments = 0
+        self.indexedDocuments = 0
+    
 
 class SearchForm(MaayPage):
     &quot;&quot;&quot;default search form&quot;&quot;&quot;
@@ -333,7 +299,7 @@
         IndexationPage.querier = self.querier
         indexationPage.updateDocumentStats()
         
-        if start == 0:
+	if start == 0:
             if indexer.is_running():
                 msg = &quot;Indexer running&quot;
             else:
@@ -371,16 +337,18 @@
         &quot;&quot;&quot;download distant file and put it in a public indexable directory&quot;&quot;&quot;
         host = context.arg('host')
         port = context.arg('port')
+        queryId = context.arg('qid')
         words = context.arg('words').split()
         filename = context.arg('filename')
         docid = context.arg('docid')
         if not host or not port or not docid:
             return Maay404()
-        print &quot;SearchForm distantfile&quot;
+        self.proivderSet = set()
         proxy = Proxy('<A HREF="http://%s:%s">http://%s:%s</A>' % (host, port))
         d = proxy.callRemote('downloadFile', docid, words)
         d.addCallback(self.gotDataBack, filename)
-        d.addErrback(self.onDownloadFileError, filename)
+        d.addErrback(self.tryOtherProviders, filename, words, host,
+                     port, docid, queryId)
         return d
 
     def gotDataBack(self, rpcFriendlyData, filename):
@@ -395,7 +363,25 @@
     def onDownloadFileError(self, error, filename):
         msg = &quot;Error while downloading file: %s&quot; % (filename,)
         return Maay404(msg)
+
+    def tryOtherProviders(self, error, filename, words, host, port, docId, queryId):
+        &quot;&quot;&quot;starts to explore the list of other providers&quot;&quot;&quot;
+        providers = self.querier.getProvidersFor(docId, queryId)
+        self.providerSet.remove((host, port))
+        return self.retryWithOtherProvider(words, docId, filename)
     
+    def retryWithOtherProvider(self, words, docId, filename):
+        if self.providerSet:
+            nextHost, nextPort = self.providerSet.pop()
+            proxy = Proxy('<A HREF="http://%s:%s">http://%s:%s</A>' % (nextHost, nextPort))
+            d = proxy.callRemote('downloadFile', docId, words)
+            d.addCallback(self.gotDataBack, filename)
+            d.addErrback(self.retryWithOtherProvider, words, docId, filename)
+            return d
+        else:
+            return self.onDownloadFileError('no provider available', filename)
+    
+    
 class DistantFilePage(static.File):
     def __init__(self, filepath):
         static.File.__init__(self, filepath)
@@ -507,6 +493,7 @@
             baseurl += '&amp;port=%s' % (document.port,)
         baseurl += '&amp;filename=%s' % osp.basename(document.url)
         baseurl += '&amp;words=%s' % '+'.join(self.query.words)
+        baseurl += '&amp;qid=%s' % (self.queryId,)
         context.fillSlots('url', baseurl)
         return context.tag
     


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000572.html">[Maay-svncheckins] r600 - trunk
</A></li>
	<LI>Next message: <A HREF="000574.html">[Maay-svncheckins] r602 - in trunk: . maay/configuration/win32
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#573">[ date ]</a>
              <a href="thread.html#573">[ thread ]</a>
              <a href="subject.html#573">[ subject ]</a>
              <a href="author.html#573">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/maay-svncheckins">More information about the Maay-svncheckins
mailing list</a><br>
</body></html>
